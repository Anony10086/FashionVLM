wandb:
  entity: null
  resume: auto
  run_id: 9xdpw3ji
experiment:
  project: tuning
  name: show-o-tuning-stage2-512x512
  output_dir: /mnt/d/PostDoc/fifth paper/code/FashionVLM/show_o/outputs/FashionVLM-2025-03-30
  max_train_examples_t2i: 129073
  max_train_examples_mmu: 302672
  save_every: 10000
  eval_every: 10000
  generate_every: 10000
  log_every: 100
  log_grad_norm_every: 500
  resume_from_checkpoint: null
  logging_dir: outputs/show-o-tuning-stage2-512x512/logs
model:
  vq_model:
    type: magvitv2
    vq_model_name: showlab/magvitv2
  showo:
    load_from_showo: true
    pretrained_model_path: showlab/show-o-512x512
    w_clip_vit: false
    vocab_size: 58498
    llm_vocab_size: 50295
    llm_model_path: microsoft/phi-1_5
    codebook_size: 8192
    num_vq_tokens: 1024
    num_new_special_tokens: 10
  gradient_checkpointing: true
dataset:
  gen_type: t2i
  und_type: llava_tuning
  combined_loader_mode: min_size
  add_system_prompt: false
  params:
    train_t2i_shards_path_or_url:
    - /workspace/FashionVLM/datasets/FashionRec/data/fashion_image_generation/{000..013}.tar
    train_mmu_shards_path_or_url:
    - /workspace/FashionVLM/datasets/FashionRec/data/basic_recommendation/{000..008}.tar
    - /workspace/FashionVLM/datasets/FashionRec/data/personalized_recommendation/{000..020}.tar
    - /workspace/FashionVLM/datasets/FashionRec/data/alternative_recommendation/000.tar
    train_lm_shards_path_or_url: /workspace/FashionVLM/datasets/falcon-refinedweb/data/*.parquet
    add_caption_prompt: false
    external_journeydb_caption_path: ''
    validation_prompts_file: validation_prompts/showoprompts.txt
    shuffle_buffer_size: 1000
    num_workers: 8
    resolution: 512
    pin_memory: true
    persistent_workers: true
  preprocessing:
    max_seq_length: 381
    resolution: 512
    center_crop: false
    random_flip: false
optimizer:
  name: adamw
  params:
    learning_rate: 5.0e-05
    scale_lr: false
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.01
    epsilon: 1.0e-08
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 1000
training:
  gradient_accumulation_steps: 1
  noise_type: mask
  batch_size_t2i: 3
  batch_size_lm: 1
  batch_size_mmu: 2
  mixed_precision: bf16
  enable_tf32: true
  seed: 10086
  max_train_steps: 200000
  overfit_one_batch: false
  cond_dropout_prob: 0.1
  min_masking_rate: 0.0
  label_smoothing: 0.0
  max_grad_norm: null
  guidance_scale: 5.0
  generation_timesteps: 50
  t2i_coeff: 1.0
  lm_coeff: 0.1
  mmu_coeff: 1.0
config: configs_docker/showo_instruction_tuning_2_512x512.yaml
